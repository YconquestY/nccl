ncclFunc_t {
    shape: class

    ncclFuncBroadcast: 0
    ncclFuncReduce: 1
    ncclFuncAllGather: 2
    ncclFuncReduceScatter: 3
    ncclFuncAllReduce: 4
    ncclFuncSendRecv: 5
    ncclFuncSend: 6
    ncclFuncRecv: 7
    ncclNumFuncs: 8
}
ncclDataType_t {
    shape: class
}
ncclComm {
    shape: class

    uint64_t: startMagic
    struct ncclMemoryStack: memPermanent, memScoped
    struct ncclSharedResources\*: sharedRes
    struct ncclChannel: channels\[MAXCHANNELS\]
    struct ncclTopoSystem\*: topo
    uint64_t\*: connectSend, connectRecv # Bitmasks for `ncclTransportP2pSetup`
    int: rank # My rank in the communicator
    int\': cudaDev # My CUDA device index
    int\'\': nNodes
    int\'\'\': maxLocalRanks
    bool: checkPointers
    int\'\'\'\': nChannels, p2pnChannels
    ncclResult_t: asyncResult # For NCCL asynchronous operations
    struct ncclComm\*: intraComm0 # Leader of intra-process `comm`s (self possible)
    struct ncclProxyState\*: proxyState
    # `groupNext` holds 0x1 when not yet in a group.
    # `preconnectNext` is a subset of communicators in `groupNext`.
    struct ncclComm\*\': groupNext, preconnectNext
    struct P2pSchedulePair\*: p2pSchedule
    struct ncclKernelPlanner: planner
    ncclConfig_t: config
    struct ncclGroupJob\*: groupJob
    …
}
ncclComm -> ncclSharedResources: sharedRes
ncclComm -> ncclTopoSystem: topo
ncclComm -> ncclChannel: channels {
    style: {
        stroke-dash: 6
    }
}
ncclComm -> ncclKernelPlanner: planner {
    style: {
        stroke-dash: 6
    }
}
ncclComm -> ncclComm: groupNext, intraComm0
ncclComm -> ncclMemoryStack: memPermanent, memScoped {
    style: {
        stroke-dash: 6
    }
}
ncclComm -> ncclConfig_v21700: config {
    style: {
        stroke-dash: 6
    }
}
ncclComm -> P2pSchedulePair: p2pSchedule # Array
ncclComm -> ncclProxyState: proxyState
ncclComm -> ncclGroupJob: groupJob

ncclSharedResources {
    shape: class

    int: refCount
    struct ncclComm\*: owner
    …
}
ncclSharedResources -> ncclComm: owner

ncclTopoSystem {
    shape: class

    int: ststemId
    uint64_t: hostHashes\[NCCL_TOPO_MAX_NODES\]
    int\': nHosts
    struct ncclTopoNodeSet: nodes\[NCCL_TOPO_NODE_TYPES\]
    float: maxBw
    float\': totalBw
}
ncclTopoSystem -> ncclTopoNodeSet: nodes {
    style: {
        stroke-dash: 6
    }
}
ncclTopoNodeSet: {
    shape: class

    int: count
    struct ncclTopoNode: nodes\[NCCL_TOPO_NODE_MAX\]
}
ncclTopoNodeSet -> ncclTopoNode: nodes {
    style: {
        stroke-dash: 6
    }
}
ncclTopoNode: {
    shape: class
    …
}
ncclChannel {
    shape: class

    struct ncclChannelPeer\*\*: peers
    …
}
ncclChannel -> ncclChannelPeer: peers {
    style: {
        stroke-dash: 6
    }
}
ncclChannelPeer {
    shape: class

    struct ncclConnector: send\[NCCL_MAX_CONNS\]
    struct ncclConnector\': recv\[NCCL_MAX_CONNS\]
    int: refCount # Similar to `std::shared_ptr`?
}
ncclChannelPeer -> ncclConnector: send, recv {
    style: {
        stroke-dash: 6
    }
}
ncclConnector {
    shape: class
    int: connected
}
ncclKernelPlanner {
    shape: class

    struct Peer
    struct Peer\*: peers
    int: nTasksP2p
    struct ncclCudaStreamList\*: streams # User streams from all tasks
    cudaStream_t: streamRecent # Most recent user stream
    struct ncclCudaGraph: capturingGraph
    …
}
ncclKernelPlanner -> Peer
ncclKernelPlanner -> Peer: peers
ncclKernelPlanner -> ncclCudaStreamList: streams {
    style: {
        stroke-dash: 6
    }
}
ncclKernelPlanner -> ncclCudaGraph: capturingGraph {
    style: {
        stroke-dash: 6
    }
}
Peer {
    shape: class

    bool: sendSeen, recvSeen
    struct ncclInstruQueue: sendQueue, recvQueue
}
Peer -> ncclInstruQueue: sendQueue, recvQueue {
    style: {
        stroke-dash: 6
    }
}
ncclInstruQueue {
    shape: class
    T\*: head, tail
}
ncclInstruQueue -> ncclInstruQueue: head, tail
ncclInstruQueue -> ncclTaskP2p: T {
    style: {
        stroke-dash: 6
    }
}
ncclInstruQueue -> ncclAsyncJob: T {
    style: {
        stroke-dash: 6
    }
}
ncclTaskP2p {
    shape: class

    struct ncclTaskP2p\*: next
    void\*: buff
    size_t: bytes
}
ncclTaskP2p -> ncclTaskP2p: next

ncclConfig_v21700 {
    shape: class
    int: blocking
    …
}
ncclInfo {
    shape: class

    ncclFunc_t: coll
    void\*: recvbuff
    size_t: count
    ncclDataType_t: datatype
    int: root # Peer for P2P operations
    ncclComm_t: comm
    cudaStream_t: stream
    …
}
ncclInfo -> ncclComm: comm
ncclInfo -> ncclFunc_t: coll {
    style: {
        stroke-dash: 6
    }
}
ncclInfo -> ncclDataType_t: datatype {
    style: {
        stroke-dash: 6
    }
}
ncclCommInitRankAsyncJob {
    shape: class

    struct ncclAsyncJob: base
    struct ncclComm\*: comm
    struct ncclComm\*\*: newcomm
    int: cudaDev
    # For `ncclCommInitRank`
    int\': nranks, myrank;
    ncclUniqueId: commId;
    # For `ncclCommSplit`
    struct ncclComm\*: parent
    int\'\': color, key
}
ncclCommInitRankAsyncJob -> ncclAsyncJob: base {
    style: {
        stroke-dash: 6
    }
}
ncclCommInitRankAsyncJob -> ncclComm: parent, newcomm

ncclPreconnectJob: {
    shape: class

    struct ncclAsyncJob: base
    struct ncclComm\*: comm
    bool\*: algoNeedConnect
}
ncclPreconnectJob -> ncclAsyncJob: base {
    style: {
        stroke-dash: 6
    }
}
ncclPreconnectJob -> ncclComm: comm
ncclAsyncJob -> ncclComm: comm {
    style: {
        stroke-dash: 6
    }
}
ncclAsyncJob {
    shape: class
    near: top-center

    struct ncclAsyncJob\*: next
    pthread_t: thread
    ncclResult_t: result
    struct ncclAsyncJob\* → ncclResult_t: func
    struct ncclAsyncJob\* → void: undo
    void\* → void: destructor
    ncclGroupJobState_t: state
    uint32_t\*: abortFlag
    uint32_t\*: abortFlagDev
    uint32_t\*: childAbortFlag
    uint32_t\*: childAbortFlagDev
    ncclComm_t: comm
    int: destroyFlag
}


ncclGroupJob {
    shape: class

    struct ncclAsyncJob: base
    struct ncclComm\*\*: groupCommHeadPtr
    struct ncclComm\*\*\': groupCommPreconnectHeadPtr
    ncclResult_t\*: groupErrorPtr
    bool\*: abortFlagPtr
    int\*: groupBlockingPtr
    struct ncclIntruQueue\*: asyncJobsPtr
    bool: initialized
}
ncclGroupJob -> ncclAsyncJob: base {
    style: {
        stroke-dash: 6
    }
}
ncclGroupJob -> ncclComm: groupCommPreconnectHeadPtr
ncclGroupJob -> ncclInstruQueue: asyncJobsPtr

ncclMemoryStack {
    shape: class

    struct Hunk
    struct Unhunk
    struct Frame
    allocateSpilled(…): void*
    allocate(…): void*
    struct Hunk\': stub
    struct Frame\': topFrame
}
ncclMemoryStack -> Hunk
ncclMemoryStack -> Hunk: stub {
    style: {
        stroke-dash: 6
    }
}
ncclMemoryStack -> Unhunk
ncclMemoryStack -> Unhunk: topFrame {
    style: {
        stroke-dash: 6
    }
}
ncclMemoryStack -> Frame
Hunk {
    shape: class

    struct Hunk\*: above # Reverse stack pointer?
    size_t: size
}
Hunk -> Hunk: above

Unhunk {
    shape: class

    struct Unhunk\*: next
    void\*: obj
}
Unhunk -> Unhunk: next

Frame {
    shape: class
    
    struct Hunk\*: hunk # Top of non-empty hunks
    uintptr_t: bumper, end
    struct Unhunk\*: unhunk
    struct Frame\*: below
}
Frame -> Hunk: hunk
Frame -> Unhunk: unhunk
Frame -> Frame: below

P2pSchedulePair {
    shape: class
    int: sendRank, recvRank
}
ncclCudaStreamList {
    shape: class

    struct ncclCudaStreamList\*: next
    cudaStream_t: stream
}
ncclCudaStreamList -> ncclCudaStreamList: next

ncclCudaGraph {
    shape: class
    cudaGraph_t: graph
    unsigned long long: graphId
}

ncclProxyState {
    shape: class
    ncclResult_t: proxyResult
}
